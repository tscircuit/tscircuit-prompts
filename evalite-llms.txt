From https://github.com/mattpocock/evalite
under MIT

This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: apps/evalite-docs/src/content/**/*
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
apps/
  evalite-docs/
    src/
      content/
        docs/
          examples/
            ai-sdk.md
          guides/
            cli.mdx
            customizing-the-ui.mdx
            environment-variables.mdx
            multi-modal.mdx
            scorers.mdx
            skipping.mdx
            streams.md
            traces.mdx
          index.mdx
          quickstart.mdx
          what-is-evalite.mdx
        config.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="apps/evalite-docs/src/content/docs/examples/ai-sdk.md">
---
title: AI SDK
---

Vercel's [AI SDK](https://sdk.vercel.ai/docs/introduction) is a great way to get started with AI in your apps.

It abstracts away the differences between different AI providers, so you can **switch between them easily**.

## Tracing

You can use the `traceAISDKModel` function to trace the calls to the AI SDK:

```ts
// my-eval.eval.ts

import { openai } from "@ai-sdk/openai";
import { streamText } from "ai";
import { Factuality, Levenshtein } from "autoevals";
import { evalite } from "evalite";
import { traceAISDKModel } from "evalite/ai-sdk";

evalite("Test Capitals", {
  data: async () => [
    {
      input: `What's the capital of France?`,
      expected: `Paris`,
    },
    {
      input: `What's the capital of Germany?`,
      expected: `Berlin`,
    },
  ],
  task: async (input) => {
    const result = streamText({
      model: traceAISDKModel(openai("gpt-4o-mini")),
      system: `
        Answer the question concisely. Answer in as few words as possible.
        Remove full stops from the end of the output.
        If the country has no capital, return '<country> has no capital'.
        If the country does not exist, return 'Unknown'.
      `,
      prompt: input,
    });

    return await result.text;
  },
  scorers: [Factuality, Levenshtein],
});
```

## Testing Whole Conversations

You can also pass messages to the `input` property of the eval. To get autocomplete, you can pass the `CoreMessage` type to the `evalite` function as a type argument.

The three type parameters for `evalite` are:

- The type of the input
- The type of the output
- The type of the expected output (optional)

```ts
// my-eval.eval.ts

import { openai } from "@ai-sdk/openai";
import { streamText, type CoreMessage } from "ai";
import { Levenshtein } from "autoevals";
import { evalite } from "evalite";
import { traceAISDKModel } from "evalite/ai-sdk";

evalite<CoreMessage[], string, string>("Test Capitals", {
  data: async () => [
    {
      input: [
        {
          content: `What's the capital of France?`,
          role: "user",
        },
      ],
      expected: `Paris`,
    },
  ],
  task: async (input) => {
    const result = streamText({
      model: traceAISDKModel(openai("gpt-4o-mini")),
      system: `
        Answer the question concisely. Answer in as few words as possible.
        Remove full stops from the end of the output.
        If the country has no capital, return '<country> has no capital'.
        If the country does not exist, return 'Unknown'.
      `,
      messages: input,
    });

    return await result.text;
  },
  scorers: [Levenshtein],
});
```
</file>

<file path="apps/evalite-docs/src/content/docs/guides/cli.mdx">
---
title: CLI
---

## Watch Mode

You can run Evalite in watch mode by running `evalite watch`:

```bash
evalite watch
```

This will watch for changes to your `.eval.ts` files and re-run the evals when they change.

> [!IMPORTANT]
>
> I strongly recommend implementing a caching layer in your LLM calls when using watch mode. This will keep your evals running fast and avoid burning through your API credits.

## Running Specific Files

You can run specific files by passing them as arguments:

```bash
evalite my-eval.eval.ts
```

This also works for `watch` mode:

```bash
evalite watch my-eval.eval.ts
```

## Threshold

You can tell Evalite that your evals must pass a specific score by passing `--threshold`:

```bash
evalite --threshold=50 # Score must be greater than or equal to 50

evalite watch --threshold=70 # Also works in watch mode
```

This is useful for running on CI. If the score threshold is not met, it will fail the process.
</file>

<file path="apps/evalite-docs/src/content/docs/guides/customizing-the-ui.mdx">
---
title: Customizing The UI
---

import { Aside } from "@astrojs/starlight/components";

## Creating Custom Columns

By default, the Evalite UI renders the input, expected and output columns:

| Input                    | Expected                    | Output           |
| ------------------------ | --------------------------- | ---------------- |
| `input` passed to `data` | `expected` passed to `data` | Result of `task` |

You can customize the columns shown by the UI by passing a `columns` attribute to the `evalite` function:

```ts
import { evalite } from "evalite";

evalite("My Eval", {
  data: async () => {
    return [
      { input: { a: 1, b: 2, c: 3, theOnlyPropertyWeWantToShow: "Hello" } },
    ];
  },
  task: async (input) => {
    return input.theOnlyPropertyWeWantToShow + " World!";
  },
  scorers: [],
  columns: async (result) => {
    return [
      {
        label: "Custom Input",
        value: result.input.theOnlyPropertyWeWantToShow, // "Hello"
      },
      {
        label: "Output",
        value: result.output, // "Hello World!"
      },
    ];
  },
});
```

This will show two columns:

| Custom Input | Output       |
| ------------ | ------------ |
| Hello        | Hello World! |
</file>

<file path="apps/evalite-docs/src/content/docs/guides/environment-variables.mdx">
---
title: Environment Variables
---

import { Steps } from "@astrojs/starlight/components";

To call your LLM from a third-party service, you'll likely need some environment variables to keep your API keys safe.

Since **Evalite is based on Vitest**, it should already pick them up from your `vite.config.ts`.

## Setting Up Env Variables

If you don't have Vitest set up, here's how to do it:

<Steps>

1. Create a `.env` file in the root of your project:

   ```
   // .env
   OPENAI_API_KEY=your-api-key
   ```

2. Add `.env` to your `.gitignore`, if it's not already there

   ```
   // .gitignore
   .env
   ```

3. Install `dotenv`:

   ```bash
   pnpm add -D dotenv
   ```

4. Add a `vite.config.ts` file:

   ```ts
   // vite.config.ts

   import { defineConfig } from "vite";

   export default defineConfig({
     test: {
       setupFiles: ["dotenv/config"],
     },
   });
   ```

</Steps>

Now, your environment variables will be available in your evals.
</file>

<file path="apps/evalite-docs/src/content/docs/guides/multi-modal.mdx">
---
title: Multi-Modal
---

import { Aside } from "@astrojs/starlight/components";

Evalite can handle not just text responses, but media like images, audio, and video.

## Files In Memory

A common way to work with media in Evalite is to read it into memory.

### What Are Files In Memory?

If you've got a file in memory, it's probably going to be in a buffer, or a `Uint8Array`. For instance, in Node, calling `readFileSync` will return a `Buffer`:

```ts
import { readFileSync } from "fs";

const fileContents = readFileSync("path/to/file.jpg");
//    ^ Buffer
```

This `Buffer` contains all the information about the file, stored in memory. You can write it to another file like so:

```ts
import { writeFileSync } from "fs";

writeFileSync("path/to/new-file.jpg", fileContents);
```

It doesn't matter what the file extension is - when you read it into memory, it'll be a `Buffer`.

<Aside>
  A `Buffer` is just a `Uint8Array` with some extra methods, so from now on I'll
  just refer to them as `Uint8Array` objects.
</Aside>

### Evalite And Files In Memory

Evalite can automatically detect `Uint8Array` objects in your evals and handle them for you.

```ts
import { evalite } from "evalite";
import { reportTrace } from "evalite/traces";

evalite("My Eval", {
  data: async () => {
    return [
      {
        // 1. In inputs...
        input: readFileSync("path/to/file.jpg"),
        // 2. ...in expected...
        expected: readFileSync("path/to/file.jpg"),
      },
    ];
  },
  task: async (input) => {
    reportTrace({
      // 3. ...in traces...
      input: readFileSync("path/to/file.jpg"),
      output: readFileSync("path/to/file.jpg"),
    });

    // 4. ...returned from the task itself...
    return readFileSync("path/to/new-file.jpg", input);
  },
  columns: async () => {
    return [
      {
        label: "File",
        // 5. ...and returned from customColumns:
        value: readFileSync("path/to/new-file.jpg", input),
      },
    ];
  },
  scorers: [],
});
```

When Evalite finds a `Uint8Array`, it saves the file to a local cache, in `./node_modules/.evalite/files`.

Then in the UI, it'll reference that local file.

## Files On Disk

If you're working with files on disk, you can use the `EvaliteFile.fromPath` method to reference them:

```ts
import { EvaliteFile, evalite } from "evalite";

evalite("My Eval", {
  data: async () => {
    return [
      {
        input: EvaliteFile.fromPath("path/to/file.jpg"),
      },
    ];
  },
  task: async (input) => {
    console.log(input.path); // "path/to/file.jpg"
  },
  scorers: [],
});
```

This lets you display an image in the UI without having to read it into memory.
</file>

<file path="apps/evalite-docs/src/content/docs/guides/scorers.mdx">
---
title: Scorers
---

import { Aside } from "@astrojs/starlight/components";

Scorers are used to score the output of your LLM call.

[Autoevals](https://github.com/braintrustdata/autoevals) is a great library of scorers to get you started.

## Inline Scorers

If you don't need your scorer to be reusable, you can define it inline.

```ts
import { evalite } from "evalite";

evalite("My Eval", {
  data: async () => {
    return [{ input: "Hello" }];
  },
  task: async (input) => {
    return input + " World!";
  },
  scorers: [
    {
      name: "Contains Paris",
      description: "Checks if the output contains the word 'Paris'.",
      scorer: ({ output }) => {
        return output.includes("Paris") ? 1 : 0;
      },
    },
  ],
});
```

## Creating Reusable Scorers

If you have a scorer you want to use across multiple files, you can use `createScorer` to create a reusable scorer.

```ts
import { createScorer } from "evalite";

const containsParis = createScorer<string, string>({
  name: "Contains Paris",
  description: "Checks if the output contains the word 'Paris'.",
  scorer: ({ output }) => {
    return output.includes("Paris") ? 1 : 0;
  },
});

evalite("My Eval", {
  data: async () => {
    return [{ input: "Hello" }];
  },
  task: async (input) => {
    return input + " World!";
  },
  scorers: [containsParis],
});
```

The `name` and `description` of the scorer will be displayed in the Evalite UI.

## Score Properties

The `score` function receives three properties on the object passed:

```ts
import { createScorer } from "evalite";

const containsParis = createScorer<string, string>({
  name: "Contains Paris",
  description: "Checks if the output contains the word 'Paris'.",
  scorer: ({ input, output, expected }) => {
    // input comes from `data`
    // expected also comes from `data`
    // output is the output of `task`
    return output.includes("Paris") ? 1 : 0;
  },
});
```

These are typed using the three type arguments passed to `createScorer`:

```ts
import { createScorer } from "evalite";

const containsParis = createScorer<
  string, // Type of 'input'
  string, // Type of 'output'
  string // Type of 'expected'
>({
  name: "Contains Word",
  description: "Checks if the output contains the specified word.",
  scorer: ({ output, input, expected }) => {
    // output is typed as string!
    return output.includes(expected) ? 1 : 0;
  },
});
```

If `expected` is omitted, it will be inferred from the type of `output`.

## Scorer Metadata

You can provide metadata along with your custom scorer:

```ts
import { createScorer } from "evalite";

const containsParis = createScorer<string>({
  name: "Contains Paris",
  description: "Checks if the output contains the word 'Paris'.",
  scorer: (output) => {
    return {
      score: output.includes("Paris") ? 1 : 0,
      metadata: {
        // Can be anything!
      },
    };
  },
});
```

This will be visible along with the score in the Evalite UI.

<Aside type="tip">

This is especially useful for debugging LLM-as-a-judge evals. In autoevals `Factuality` scorer, the metadata will include a rationale for why the scorer gave the score it did.

</Aside>

## Creating LLM-As-A-Judge Scorers

Here is a brief guide on building your own LLM-as-a-judge scorer.

We're looking to improve this feature with a first-class guide in the future.

```ts
import { openai } from "@ai-sdk/openai";
import { generateObject } from "ai";
import { createScorer } from "evalite";
import { z } from "zod";

/**
 * Factuality scorer using OpenAI's GPT-4o model.
 */
export const Factuality = createScorer<string, string, string>({
  name: "Factuality",
  scorer: async ({ input, expected, output }) => {
    return checkFactuality({
      question: input,
      groundTruth: expected!,
      submission: output,
    });
  },
});

/**
 * Checks the factuality of a submission, using
 * OpenAI's GPT-4o model.
 */
const checkFactuality = async (opts: {
  question: string;
  groundTruth: string;
  submission: string;
}) => {
  const { object } = await generateObject({
    model: openai("gpt-4o-2024-11-20"),
    /**
     * Prompt taken from autoevals:
     *
     * {@link https://github.com/braintrustdata/autoevals/blob/5aa20a0a9eb8fc9e07e9e5722ebf71c68d082f32/templates/factuality.yaml}
     */
    prompt: `
      You are comparing a submitted answer to an expert answer on a given question. Here is the data:
      [BEGIN DATA]
      ************
      [Question]: ${opts.question}
      ************
      [Expert]: ${opts.groundTruth}
      ************
      [Submission]: ${opts.submission}
      ************
      [END DATA]

      Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.
      The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:
      (A) The submitted answer is a subset of the expert answer and is fully consistent with it.
      (B) The submitted answer is a superset of the expert answer and is fully consistent with it.
      (C) The submitted answer contains all the same details as the expert answer.
      (D) There is a disagreement between the submitted answer and the expert answer.
      (E) The answers differ, but these differences don't matter from the perspective of factuality.
    `,
    schema: z.object({
      answer: z.enum(["A", "B", "C", "D", "E"]).describe("Your selection."),
      rationale: z
        .string()
        .describe("Why you chose this answer. Be very detailed."),
    }),
  });

  /**
   * LLM's are well documented at being poor at generating
   */
  const scores = {
    A: 0.4,
    B: 0.6,
    C: 1,
    D: 0,
    E: 1,
  };

  return {
    score: scores[object.answer],
    metadata: {
      rationale: object.rationale,
    },
  };
};

/**
 * Use the Factuality eval like so:
 *
 * 1. The input (in data()) is a question.
 * 2. The expected output is the ground truth answer to the question.
 * 3. The output is the text to be evaluated.
 */
evalite("Factuality", {
  data: async () => {
    return [
      {
        // The question
        input: "What is the capital of France?",

        // The expected answer
        expected: "Paris",
      },
    ];
  },
  task: async (input) => {
    // Technically correct, but a nightmare for non-LLM
    // scorers to evaluate.
    return (
      "The capital of France is a city that starts " +
      "with a letter P, and ends in 'aris'."
    );
  },
  scorers: [Factuality],
});
```
</file>

<file path="apps/evalite-docs/src/content/docs/guides/skipping.mdx">
---
title: Skipping Evals
---

Evalite has an `experimental_skip` modifier that you can use to skip a test.

```ts
evalite.experimental_skip("My Eval", {
  data: () => [],
  task: () => {},
});
```

This will skip the test and not run it.

This is currently in an experimental state, since it doesn't yet display in the Evalite UI.
</file>

<file path="apps/evalite-docs/src/content/docs/guides/streams.md">
---
title: Streams
---

You can handle streams in Evalite by returning any async iterable (including a `ReadableStream`) from your task. This means you can test functions like the AI SDK `streamText` function easily:

```ts
import { evalite } from "evalite";
import { streamText } from "ai";
import { openai } from "@ai-sdk/openai";
import { Factuality } from "autoevals";

evalite("My Eval", {
  data: async () => {
    return [{ input: "What is the capital of France?", expected: "Paris" }];
  },
  task: async (input) => {
    const result = await streamText({
      model: openai("your-model"),
      system: `Answer the question concisely.`,
      prompt: input,
    });

    return result.textStream;
  },
  scorers: [Factuality],
});
```
</file>

<file path="apps/evalite-docs/src/content/docs/guides/traces.mdx">
---
title: Traces
---

import { Aside } from "@astrojs/starlight/components";

Traces are used to track the behaviour of each individual call to an LLM inside your task.

## `reportTrace`

You can report a trace by calling `reportTrace` inside an `evalite` eval:

```ts
import { evalite, type Evalite } from "evalite";
import { reportTrace } from "evalite/traces";

evalite("My Eval", {
  data: async () => {
    return [{ input: "Hello", expected: "Hello World!" }];
  },
  task: async (input) => {
    // Track the start time
    const start = performance.now();

    // Call our LLM
    const result = await myLLMCall();

    // Report the trace once it's finished
    reportTrace({
      start,
      end: performance.now(),
      output: result.output,
      input: [
        {
          role: "user",
          content: input,
        },
      ],
      usage: {
        completionTokens: result.completionTokens,
        promptTokens: result.promptTokens,
      },
    });

    // Return the output
    return result.output;
  },
  scorers: [Levenshtein],
});
```

<Aside>

`reportTrace` is a no-op in production, so you can leave it in your code without worrying about performance.

</Aside>

## `traceAISDKModel`

If you're using the [Vercel AI SDK](https://sdk.vercel.ai/docs/introduction), you can automatically report traces by wrapping your model in `traceAISDKModel` function:

```ts
import { traceAISDKModel } from "evalite/ai-sdk";
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

// All calls to this model will be recorded in evalite!
const tracedModel = traceAISDKModel(openai("gpt-4o-mini"));

const result = await generateText({
  model: tracedModel,
  system: `Answer the question concisely.`,
  prompt: `What is the capital of France?`,
});
```

<Aside>

`traceAISDKModel`, like `reportTrace`, is a no-op in production.

</Aside>
</file>

<file path="apps/evalite-docs/src/content/docs/index.mdx">
---
title: Test GenAI-powered apps in TypeScript
description: Evalite makes evals simple. Test your AI-powered apps with a local dev server. No API key required.
template: splash
hero:
  tagline: Evals are <b>hard</b>. Evalite makes them simple. Test your AI-powered apps with a local dev server. No API key required.
  image:
    html: <img src="/hero.webp" style="width:min(100%,25rem);" alt="" aria-hidden="true" loading="eager" />
  actions:
    - text: Get Started
      link: /quickstart/
      icon: right-arrow
---

import { Card, CardGrid } from "@astrojs/starlight/components";

## Features

<CardGrid stagger>
  <Card title="Just A Test Runner" icon="laptop">
    `.eval.ts` is the new `.test.ts`. Write evals in TypeScript with an
    instantly familiar API.
  </Card>
  <Card title="Instant Feedback" icon="rocket">
    Explore your outputs, traces and logs in a beautiful, interactive UI that
    runs on `localhost`.
  </Card>
  <Card title="Built on Vitest" icon="laptop">
    Works out of the box with your stack. Use mocks, fixtures, and more to
    simulate real-world scenarios.
  </Card>
  <Card title="No Vendor Lock-In" icon="puzzle">
    Work with any LLM, and test outputs against each other seamlessly.
  </Card>
  <Card title="Put Away Your Credit Card" icon="approve-check-circle">
    No cloud platform. No API key. Your data stays on your machine. No sign-off
    needed.
  </Card>
</CardGrid>
</file>

<file path="apps/evalite-docs/src/content/docs/quickstart.mdx">
---
title: Quickstart
description: A guide in my new Starlight docs site.
---

import { Aside, Steps } from "@astrojs/starlight/components";

We're going to walk through setting up Evalite in an existing project.

<Steps>

1. Install `evalite`, `vitest`, and a scoring library like `autoevals`:

   ```bash
   pnpm add -D evalite vitest autoevals
   ```

2. Add an `eval:dev` script to your package.json:

   ```json
   {
     "scripts": {
       "eval:dev": "evalite watch"
     }
   }
   ```

3. Create your first eval:

   ```ts
   // my-eval.eval.ts

   import { evalite } from "evalite";
   import { Levenshtein } from "autoevals";

   evalite("My Eval", {
     // A function that returns an array of test data
     // - TODO: Replace with your test data
     data: async () => {
       return [{ input: "Hello", expected: "Hello World!" }];
     },
     // The task to perform
     // - TODO: Replace with your LLM call
     task: async (input) => {
       return input + " World!";
     },
     // The scoring methods for the eval
     scorers: [Levenshtein],
   });
   ```

   <Aside type="tip">

   `.eval.ts` is the extension Evalite looks for when scanning for evals.

   </Aside>

4. Run `pnpm run eval:dev`.

   ```bash
   pnpm run eval:dev
   ```

   This runs `evalite`, which runs the evals:

   - Runs the `data` function to get the test data
   - Runs the `task` function on each test data
   - Scores the output of the `task` function using the `scorers`
   - Saves the results to a sqlite database in `node_modules/.evalite`

   It then:

   - Shows a UI for viewing the traces, scores, inputs and outputs at http://localhost:3006.
   - If you only ran one eval, it also shows a table summarizing the eval in the terminal.

5. Open http://localhost:3006 in your browser to view the results of the eval.

</Steps>

### What Next?

Head to the [AI SDK example](/examples/ai-sdk) to see a fully-fleshed out example of Evalite in action.

### Troubleshooting

##### Error: Could not locate the bindings file
Some users experienced issues running `evalite watch`. Your package manager will report the following error message:

```
Command failed, Error: Could not locate the bindings file.
```

This error is related to the `better-sqlite3` package. To resolve this, you can try the following steps:

-  Rebuild `better-sqlite3`:
  ```bash
  pnpm rebuild better-sqlite3
  ```

-  Approve the rebuild of the package with:
  ```bash
  pnpm approve-builds
  ```
</file>

<file path="apps/evalite-docs/src/content/docs/what-is-evalite.mdx">
---
title: What Is Evalite?
---

import { Aside, Steps } from "@astrojs/starlight/components";

Evalite runs your evals locally. Evals are like tests, but for AI-powered apps.

So Evalite is like Jest or Vitest, but for apps that use AI.

Here are the headlines:

- Lets you write evals in `.eval.ts` files.
- Runs a local server on `localhost` with live reload
- Lets you capture traces, build custom scorers, and much more
- Based on Vite & Vitest, so you can use all the same tools (mocks, lifecycle hooks, `vite.config.ts`) you're used to

## What Are Evals?

**Evals are to AI-powered apps what tests are to regular apps**. They're a way to check that your app is working as expected.

Normal tests give you a pass or fail metric. **Evals give you a score from 0-100** based on how well your app is performing.

Instead of `.test.ts` files, Evalite uses `.eval.ts` files. They look like this:

```ts
// my-eval.eval.ts

import { evalite } from "evalite";
import { Levenshtein } from "autoevals";

evalite("My Eval", {
  // A set of data to test
  data: async () => {
    return [{ input: "Hello", expected: "Hello World!" }];
  },
  // The task to perform, usually to call a LLM.
  task: async (input) => {
    return input + " World!";
  },
  // Some methods to score the eval
  scorers: [
    // For instance, Levenshtein distance measures
    // the similarity between two strings
    Levenshtein,
  ],
});
```

In the code above, we have:

- `data`: A dataset to test
- `task`: The task to perform
- `scorers`: Methods to score the eval

These are the core elements of an eval.

> Thanks to [Braintrust](https://www.braintrust.dev/) for the API inspiration

## Why Does Evalite Exist?

There are plenty of eval runners out there. But most of them are also bundled with a cloud service.

Evalite is different. It's **local-only**. It runs on your machine, and you stay in complete control of your data.

This means no friction, no sign-off, and no vendor lock-in. Just you, your code, and your evals.

And, of course, it's completely open source.
</file>

<file path="apps/evalite-docs/src/content/config.ts">
import { defineCollection } from 'astro:content';
import { docsSchema } from '@astrojs/starlight/schema';

export const collections = {
	docs: defineCollection({ schema: docsSchema() }),
};
</file>

</files>
